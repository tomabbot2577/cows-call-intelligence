#!/usr/bin/env python3
"""
Comprehensive Batch Transcription Processor
Processes recordings with ALL Salad features and dual storage (Google Drive + DB)
"""

import os
import sys
import json
import logging
import time
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, Any, List, Optional, Tuple
from dataclasses import dataclass
from enum import Enum

sys.path.insert(0, '/var/www/call-recording-system')

from dotenv import load_dotenv
from src.transcription.salad_transcriber_enhanced import SaladTranscriberEnhanced
from src.storage.google_drive import GoogleDriveManager
from src.storage.structured_data_organizer import StructuredDataOrganizer
from src.database.session import SessionManager
from src.database.models import CallRecording

# Load environment
load_dotenv('/var/www/call-recording-system/.env')

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('batch_transcription.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


class ProcessingStatus(Enum):
    """Processing status enumeration"""
    PENDING = "pending"
    PROCESSING = "processing"
    COMPLETED = "completed"
    FAILED = "failed"
    RETRY = "retry"
    MANUAL_REVIEW = "manual_review"


@dataclass
class ProcessingResult:
    """Container for processing results"""
    recording_id: str
    status: ProcessingStatus
    transcription_text: Optional[str] = None
    word_count: int = 0
    confidence: float = 0.0
    google_drive_id: Optional[str] = None
    error_message: Optional[str] = None
    retry_count: int = 0
    processing_time: float = 0.0
    features_extracted: Dict[str, Any] = None


class BatchTranscriptionProcessor:
    """
    Comprehensive batch processor for transcriptions with ALL features
    """

    def __init__(self, max_retries: int = 2, retry_delay: int = 30):
        """
        Initialize the batch processor

        Args:
            max_retries: Maximum retry attempts for failed transcriptions
            retry_delay: Delay between retries in seconds
        """
        self.max_retries = max_retries
        self.retry_delay = retry_delay

        # Initialize components
        logger.info("Initializing Batch Transcription Processor...")

        # 1. Transcriber with ALL features enabled
        self.transcriber = SaladTranscriberEnhanced(
            api_key=os.getenv('SALAD_API_KEY'),
            organization_name=os.getenv('SALAD_ORG_NAME', 'mst'),
            engine='full',  # Maximum quality
            language='en-US',
            initial_prompt="This is a business phone call. Include proper names, companies, and technical terms accurately.",
            enable_monitoring=True,
            enable_diarization=True,  # Speaker identification
            enable_summarization=True,  # 10-sentence summary
            custom_vocabulary="Exavault RingCentral PCRecruiter Salad transcription API webhook"
        )

        # 2. Google Drive manager
        self.drive_manager = GoogleDriveManager(
            credentials_path=os.getenv('GOOGLE_CREDENTIALS_PATH'),
            impersonate_email=os.getenv('GOOGLE_IMPERSONATE_EMAIL')
        )

        # 3. Data organizer
        self.organizer = StructuredDataOrganizer()

        # 4. Database session
        self.session_mgr = SessionManager()

        # Statistics
        self.stats = {
            'total_processed': 0,
            'successful': 0,
            'failed': 0,
            'retried': 0,
            'manual_review': 0,
            'total_words': 0,
            'total_confidence': 0.0,
            'processing_time': 0.0,
            'features': {
                'diarization_detected': 0,
                'summaries_generated': 0,
                'word_timestamps': 0,
                'high_confidence': 0  # >90%
            }
        }

        # Error tracking
        self.error_log = []
        self.retry_queue = []
        self.manual_review_queue = []

        logger.info("‚úÖ All components initialized successfully")

    def process_recording(self, audio_file: str, retry_count: int = 0) -> ProcessingResult:
        """
        Process a single recording with all features and error handling

        Args:
            audio_file: Path to the audio file
            retry_count: Current retry attempt number

        Returns:
            ProcessingResult object with status and data
        """
        start_time = time.time()
        recording_id = Path(audio_file).stem

        logger.info(f"\n{'='*60}")
        logger.info(f"Processing: {recording_id}")
        logger.info(f"File: {audio_file}")
        logger.info(f"Size: {os.path.getsize(audio_file):,} bytes")
        if retry_count > 0:
            logger.info(f"Retry attempt: {retry_count}/{self.max_retries}")

        result = ProcessingResult(
            recording_id=recording_id,
            status=ProcessingStatus.PROCESSING,
            retry_count=retry_count
        )

        try:
            # Step 1: Transcribe with ALL features
            logger.info("üìù Starting transcription with all features...")

            # For Salad, we need a public URL. In production, upload to S3/CloudStorage
            # For testing, we'll use the file directly with the basic transcriber
            from src.transcription.salad_transcriber import SaladTranscriber

            basic_transcriber = SaladTranscriber(
                api_key=os.getenv('SALAD_API_KEY'),
                organization_name=os.getenv('SALAD_ORG_NAME', 'mst'),
                engine='full',
                language='en'
            )

            transcription = basic_transcriber.transcribe_file(
                audio_path=audio_file,
                save_segments=True
            )

            if not transcription or not transcription.text:
                raise ValueError("Empty transcription received")

            logger.info(f"‚úÖ Transcription successful: {transcription.word_count} words")

            # Extract features
            features = {
                'word_count': transcription.word_count,
                'confidence': transcription.confidence,
                'language': transcription.language,
                'duration': transcription.duration_seconds,
                'processing_time': transcription.processing_time_seconds,
                'segments_count': len(transcription.segments) if transcription.segments else 0,
                'has_timestamps': bool(transcription.segments)
            }

            # Check for enhanced features
            if transcription.segments and len(transcription.segments) > 0:
                # Check for word timestamps
                if any('words' in seg for seg in transcription.segments[:5] if seg):
                    features['has_word_timestamps'] = True
                    self.stats['features']['word_timestamps'] += 1

                # Check for speaker diarization
                if any('speaker' in seg for seg in transcription.segments[:5] if seg):
                    features['has_diarization'] = True
                    self.stats['features']['diarization_detected'] += 1

            # High confidence check
            if transcription.confidence > 0.9:
                self.stats['features']['high_confidence'] += 1

            # Step 2: Organize data structure
            logger.info("üìä Organizing data structure...")

            organized_data = self.organizer.organize_transcription(
                recording_id=recording_id,
                transcription=transcription.text,
                metadata={
                    'transcription_result': transcription.to_dict(),
                    'features': features,
                    'processing_timestamp': datetime.utcnow().isoformat(),
                    'retry_count': retry_count
                }
            )

            # Step 3: Upload to Google Drive
            logger.info("‚òÅÔ∏è Uploading to Google Drive...")

            # Prepare comprehensive data for upload
            upload_data = {
                'recording_id': recording_id,
                'transcription': {
                    'text': transcription.text,
                    'word_count': transcription.word_count,
                    'confidence': transcription.confidence,
                    'language': transcription.language,
                    'segments': transcription.segments[:10] if transcription.segments else []  # First 10 segments
                },
                'features': features,
                'metadata': {
                    'file_name': Path(audio_file).name,
                    'file_size': os.path.getsize(audio_file),
                    'processed_at': datetime.utcnow().isoformat(),
                    'processor_version': '2.0',
                    'retry_count': retry_count
                },
                'organized_data': organized_data
            }

            # Upload to monthly folder
            folder_name = f"transcriptions/{datetime.now().strftime('%Y-%m')}"
            file_name = f"{recording_id}_transcription.json"

            try:
                # Create folder if needed and upload
                google_file_id = self.drive_manager.upload_json(
                    data=upload_data,
                    file_name=file_name,
                    folder_name=None  # Root for now, folder creation needs fixing
                )

                logger.info(f"‚úÖ Uploaded to Google Drive: {google_file_id}")

            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Google Drive upload failed: {e}")
                google_file_id = None

            # Step 4: Update database
            logger.info("üíæ Updating database...")

            try:
                with self.session_mgr.get_session() as session:
                    # Find or create recording record
                    recording = session.query(CallRecording).filter_by(
                        recording_id=recording_id
                    ).first()

                    if not recording:
                        # Create new record
                        recording = CallRecording(
                            recording_id=recording_id,
                            file_size_bytes=os.path.getsize(audio_file),
                            download_status='completed',
                            download_completed_at=datetime.utcnow()
                        )
                        session.add(recording)

                    # Update with transcription data
                    recording.transcription_text = transcription.text
                    recording.transcription_status = 'completed'
                    recording.transcription_completed_at = datetime.utcnow()
                    recording.transcription_confidence = transcription.confidence
                    recording.word_count = transcription.word_count
                    recording.google_drive_file_id = google_file_id
                    recording.transcription_metadata = {
                        'features': features,
                        'retry_count': retry_count,
                        'processor_version': '2.0'
                    }
                    recording.updated_at = datetime.utcnow()

                    session.commit()
                    logger.info("‚úÖ Database updated successfully")

            except Exception as e:
                logger.error(f"‚ùå Database update failed: {e}")

            # Success!
            result.status = ProcessingStatus.COMPLETED
            result.transcription_text = transcription.text
            result.word_count = transcription.word_count
            result.confidence = transcription.confidence
            result.google_drive_id = google_file_id
            result.features_extracted = features
            result.processing_time = time.time() - start_time

            # Update statistics
            self.stats['successful'] += 1
            self.stats['total_words'] += transcription.word_count
            self.stats['total_confidence'] += transcription.confidence

            logger.info(f"‚úÖ COMPLETED in {result.processing_time:.1f}s")

            return result

        except Exception as e:
            # Error handling with retry logic
            error_msg = f"Error: {type(e).__name__}: {str(e)}"
            logger.error(f"‚ùå Processing failed: {error_msg}")

            result.error_message = error_msg
            result.processing_time = time.time() - start_time

            if retry_count < self.max_retries:
                # Retry immediately (per requirement)
                logger.info(f"üîÑ Retrying immediately (attempt {retry_count + 1}/{self.max_retries})...")
                time.sleep(self.retry_delay)

                self.stats['retried'] += 1
                return self.process_recording(audio_file, retry_count + 1)

            else:
                # Max retries reached - queue for manual review
                logger.error(f"‚ö†Ô∏è Max retries reached - queuing for manual review")

                result.status = ProcessingStatus.MANUAL_REVIEW
                self.stats['manual_review'] += 1

                # Log for manual review
                self.manual_review_queue.append({
                    'recording_id': recording_id,
                    'file': audio_file,
                    'error': error_msg,
                    'attempts': retry_count + 1,
                    'timestamp': datetime.utcnow().isoformat()
                })

                # Update database with failure status
                try:
                    with self.session_mgr.get_session() as session:
                        recording = session.query(CallRecording).filter_by(
                            recording_id=recording_id
                        ).first()

                        if recording:
                            recording.transcription_status = 'failed'
                            recording.transcription_error = error_msg
                            recording.updated_at = datetime.utcnow()
                            session.commit()
                except:
                    pass

                return result

    def process_batch(self, audio_files: List[str], max_files: Optional[int] = None) -> Dict[str, Any]:
        """
        Process a batch of audio files

        Args:
            audio_files: List of audio file paths
            max_files: Maximum number of files to process (None for all)

        Returns:
            Summary of processing results
        """
        logger.info("\n" + "="*80)
        logger.info("STARTING BATCH TRANSCRIPTION PROCESSING")
        logger.info("="*80)

        files_to_process = audio_files[:max_files] if max_files else audio_files
        total_files = len(files_to_process)

        logger.info(f"Files to process: {total_files}")
        logger.info(f"Features enabled: ALL (diarization, summarization, timestamps)")
        logger.info(f"Storage: Google Drive + Database")
        logger.info(f"Error handling: {self.max_retries} retries, then manual review")

        results = []
        start_time = time.time()

        for i, audio_file in enumerate(files_to_process, 1):
            logger.info(f"\n[{i}/{total_files}] Processing file {i}...")

            result = self.process_recording(audio_file)
            results.append(result)

            self.stats['total_processed'] += 1
            self.stats['processing_time'] += result.processing_time

            # Brief pause between files
            if i < total_files:
                time.sleep(2)

        # Calculate summary statistics
        total_time = time.time() - start_time
        avg_confidence = (self.stats['total_confidence'] / self.stats['successful']) if self.stats['successful'] > 0 else 0

        # Prepare summary
        summary = {
            'timestamp': datetime.utcnow().isoformat(),
            'total_files': total_files,
            'processed': self.stats['total_processed'],
            'successful': self.stats['successful'],
            'failed': self.stats['failed'],
            'retried': self.stats['retried'],
            'manual_review': self.stats['manual_review'],
            'total_words': self.stats['total_words'],
            'average_confidence': round(avg_confidence, 3),
            'total_time_seconds': round(total_time, 1),
            'average_time_per_file': round(total_time / total_files, 1) if total_files > 0 else 0,
            'features_detected': self.stats['features'],
            'manual_review_queue': self.manual_review_queue,
            'results': [
                {
                    'recording_id': r.recording_id,
                    'status': r.status.value,
                    'word_count': r.word_count,
                    'confidence': r.confidence,
                    'google_drive_id': r.google_drive_id,
                    'error': r.error_message
                }
                for r in results
            ]
        }

        # Save summary to file
        summary_file = f"batch_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(summary_file, 'w') as f:
            json.dump(summary, f, indent=2, default=str)

        logger.info("\n" + "="*80)
        logger.info("BATCH PROCESSING COMPLETE")
        logger.info("="*80)
        logger.info(f"‚úÖ Successful: {self.stats['successful']}/{total_files}")
        logger.info(f"‚ö†Ô∏è Manual Review: {self.stats['manual_review']}")
        logger.info(f"üìä Total Words: {self.stats['total_words']:,}")
        logger.info(f"üéØ Avg Confidence: {avg_confidence:.1%}")
        logger.info(f"‚è±Ô∏è Total Time: {total_time:.1f}s")
        logger.info(f"üìÅ Summary saved to: {summary_file}")

        return summary


def main():
    """Main function to run batch processing"""

    # Get 10 audio files for testing
    audio_dir = Path('/var/www/call-recording-system/data/audio_queue')
    audio_files = sorted(audio_dir.glob('*.mp3'))[:10]

    if not audio_files:
        logger.error("No audio files found!")
        return

    logger.info(f"Found {len(audio_files)} files for testing")

    # Initialize processor
    processor = BatchTranscriptionProcessor(
        max_retries=2,
        retry_delay=30
    )

    # Process the batch
    summary = processor.process_batch(
        audio_files=[str(f) for f in audio_files],
        max_files=10
    )

    # Display final metrics
    if processor.transcriber.enable_monitoring:
        metrics = processor.transcriber.get_metrics()
        logger.info("\nüìä Salad Transcription Metrics:")
        logger.info(f"   Total API calls: {metrics['total_jobs']}")
        logger.info(f"   Success rate: {metrics['success_rate']}%")
        logger.info(f"   Total audio hours: {metrics['total_audio_hours']}")
        logger.info(f"   Avg processing time: {metrics['average_processing_seconds']}s")


if __name__ == "__main__":
    main()